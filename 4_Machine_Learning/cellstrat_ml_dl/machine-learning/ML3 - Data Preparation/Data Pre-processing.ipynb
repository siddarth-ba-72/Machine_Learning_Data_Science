{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### CellStrat Hub Pack - Machine Learning\n",
    "\n",
    "#### ML3 - Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ethnicity  Height (CM)  Weight (Kg) Will survive till 70\n",
      "0     White        186.0         90.0                  Yes\n",
      "1   African        185.0         98.0                   No\n",
      "2     White        175.0         80.0                   No\n",
      "3     White        180.0         88.0                  Yes\n",
      "4     Asian        178.0          NaN                   No\n",
      "5     Asian        172.0         72.0                  Yes\n",
      "6   African        178.0         75.0                   No\n",
      "7     White          NaN         89.0                  Yes\n",
      "8   African        186.0         90.0                  Yes\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "#==============================================================================\n",
    "# First step to write the python program is to take benefit out of libraries\n",
    "# already available. We will only focus on the data science related libraries.\n",
    "#==============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#==============================================================================\n",
    "# #import data from the data file. In our case its Health.csv. \n",
    "#==============================================================================\n",
    "\n",
    "healthData = pd.read_csv ('Health.csv')\n",
    "print(healthData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['White', 186.0, 90.0],\n",
       "       ['African', 185.0, 98.0],\n",
       "       ['White', 175.0, 80.0],\n",
       "       ['White', 180.0, 88.0],\n",
       "       ['Asian', 178.0, nan],\n",
       "       ['Asian', 172.0, 72.0],\n",
       "       ['African', 178.0, 75.0],\n",
       "       ['White', nan, 89.0],\n",
       "       ['African', 186.0, 90.0]], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#==============================================================================\n",
    "# All mathematical operations will be performed on the matrix, so now we create\n",
    "# matrix for dependent variables and independent variables.\n",
    "#==============================================================================\n",
    "\n",
    "\n",
    "X = healthData.iloc [:,:-1].values\n",
    "y = healthData.iloc [:,3].values\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Yes', 'No', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Value Handling with Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['White' 186.0 90.0]\n",
      " ['African' 185.0 98.0]\n",
      " ['White' 175.0 80.0]\n",
      " ['White' 180.0 88.0]\n",
      " ['Asian' 178.0 85.25]\n",
      " ['Asian' 172.0 72.0]\n",
      " ['African' 178.0 75.0]\n",
      " ['White' 180.0 89.0]\n",
      " ['African' 186.0 90.0]]\n"
     ]
    }
   ],
   "source": [
    "#==============================================================================\n",
    "# Handle the missing values, we can see that in dataset there are some missing\n",
    "# values, we will use strategy to impute mean of column values in these places\n",
    "#==============================================================================\n",
    "\n",
    "#from sklearn.preprocessing import Imputer\n",
    "# First create an Imputer\n",
    "#missingValueImputer = Imputer (missing_values = 'NaN', strategy = 'mean',\n",
    "#                               axis = 0)\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "missingValueImputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "# Set which columns imputer should perform\n",
    "missingValueImputer = missingValueImputer.fit (X[:,1:3])\n",
    "# update values of X with new values\n",
    "X[:,1:3] = missingValueImputer.transform(X[:,1:3])\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Encoding of Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 186.0 90.0]\n",
      " [0 185.0 98.0]\n",
      " [2 175.0 80.0]\n",
      " [2 180.0 88.0]\n",
      " [1 178.0 85.25]\n",
      " [1 172.0 72.0]\n",
      " [0 178.0 75.0]\n",
      " [2 180.0 89.0]\n",
      " [0 186.0 90.0]]\n"
     ]
    }
   ],
   "source": [
    "#==============================================================================\n",
    "# Encode the categorial data. So now instead of character values we will have\n",
    "# corresponding numerical values\n",
    "#==============================================================================\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "X_labelencoder = LabelEncoder()\n",
    "X[:, 0] = X_labelencoder.fit_transform(X[:, 0])\n",
    "print (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 0 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# for y\n",
    "y_labelencoder = LabelEncoder ()\n",
    "y = y_labelencoder.fit_transform (y)\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Hot Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0.0' '0.0' '1.0' '186.0' '90.0']\n",
      " ['1.0' '0.0' '0.0' '185.0' '98.0']\n",
      " ['0.0' '0.0' '1.0' '175.0' '80.0']\n",
      " ['0.0' '0.0' '1.0' '180.0' '88.0']\n",
      " ['0.0' '1.0' '0.0' '178.0' '85.25']\n",
      " ['0.0' '1.0' '0.0' '172.0' '72.0']\n",
      " ['1.0' '0.0' '0.0' '178.0' '75.0']\n",
      " ['0.0' '0.0' '1.0' '180.0' '89.0']\n",
      " ['1.0' '0.0' '0.0' '186.0' '90.0']]\n"
     ]
    }
   ],
   "source": [
    "#==============================================================================\n",
    "# Implementing OneHotEncoder to separate category variables into dummy \n",
    "# variables.\n",
    "\n",
    "#from sklearn.preprocessing import OneHotEncoder\n",
    "#X_onehotencoder = OneHotEncoder (categorical_features = [0])\n",
    "#X_onehotencoder = OneHotEncoder ()\n",
    "#X = X_onehotencoder.fit_transform(X).toarray()\n",
    "#print (X)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "columnTransformer = ColumnTransformer([('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
    "\n",
    "X = np.array(columnTransformer.fit_transform(X), dtype = str)\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Training and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# split the dataset into training and test set. We will use 80/20 approach\n",
    "#==============================================================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split (X, y, test_size = 0.2,\n",
    "                                                     random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.15470054 -0.63245553 -0.63245553  0.88159065  1.48988518]\n",
      " [-0.8660254   1.58113883 -0.63245553 -0.55834074 -0.02546812]\n",
      " [ 1.15470054 -0.63245553 -0.63245553  1.08729513  0.53907526]\n",
      " [ 1.15470054 -0.63245553 -0.63245553 -0.55834074 -1.24369333]\n",
      " [-0.8660254  -0.63245553  1.58113883 -0.14693177  0.30137279]\n",
      " [-0.8660254  -0.63245553  1.58113883  1.08729513  0.53907526]\n",
      " [-0.8660254   1.58113883 -0.63245553 -1.79256765 -1.60024704]]\n"
     ]
    }
   ],
   "source": [
    "#==============================================================================\n",
    "# Feature scaling is to bring all the independent variables in a dataset into\n",
    "# same scale, to avoid any variable dominating  the model. Here we will not \n",
    "# transform the dependent variables.\n",
    "#==============================================================================\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "independent_scalar = StandardScaler()\n",
    "X_train = independent_scalar.fit_transform (X_train) #fit and transform\n",
    "X_test = independent_scalar.transform (X_test) # only transform\n",
    "print(X_train)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
