{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellStrat Hub Pack - Deep Learning\n",
    "\n",
    "#### DL5 - Tensorflow: Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import sys\n",
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path, \n",
    "                               '%s-labels-idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, \n",
    "                               '%s-images-idx3-ubyte' % kind)\n",
    "        \n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', \n",
    "                                 lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, \n",
    "                             dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", \n",
    "                                               imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, \n",
    "                             dtype=np.uint8).reshape(len(labels), 784)\n",
    "        images = ((images / 255.) - .5) * 2\n",
    " \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 60000,  Columns: 784\n",
      "Rows: 10000,  Columns: 784\n",
      "(60000, 784) (60000,)\n",
      "(10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_mnist('./', kind='train')\n",
    "print('Rows: %d,  Columns: %d' %(X_train.shape[0], \n",
    "                                 X_train.shape[1]))\n",
    "X_test, y_test = load_mnist('./', kind='t10k')\n",
    "print('Rows: %d,  Columns: %d' %(X_test.shape[0], \n",
    "                                 X_test.shape[1]))\n",
    "\n",
    "## mean centering and normalization:\n",
    "mean_vals = np.mean(X_train, axis=0)\n",
    "std_val = np.std(X_train)\n",
    "\n",
    "X_train_centered = (X_train - mean_vals)/std_val\n",
    "X_test_centered = (X_test - mean_vals)/std_val\n",
    " \n",
    "del X_train, X_test\n",
    " \n",
    "print(X_train_centered.shape, y_train.shape)\n",
    "\n",
    "print(X_test_centered.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#import tensorflow.contrib.keras as keras\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "np.random.seed(123)\n",
    "# tf.set_random_seed(123)\n",
    "tf.compat.v1.set_random_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CellStrat - convert class labels 0-9 into one hot vector using a Keras tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 labels:  [5 0 4]\n",
      "\n",
      "First 3 labels (one-hot):\n",
      " [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "y_train_onehot = keras.utils.to_categorical(y_train)\n",
    " \n",
    "print('First 3 labels: ', y_train[:3])\n",
    "print('\\nFirst 3 labels (one-hot):\\n', y_train_onehot[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CellStrat - now define the 3 layers of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(\n",
    "    keras.layers.Dense(\n",
    "        units=50,    \n",
    "        input_dim=X_train_centered.shape[1],\n",
    "        #kernel_initializer='glorot_uniform',\n",
    "        kernel_initializer='he_normal',\n",
    "        bias_initializer='zeros',\n",
    "        activation='tanh'))\n",
    "\n",
    "model.add(\n",
    "    keras.layers.Dense(\n",
    "        units=50,    \n",
    "        input_dim=50,\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        bias_initializer='zeros',\n",
    "        activation='tanh'))\n",
    "\n",
    "model.add(\n",
    "    keras.layers.Dense(\n",
    "        units=y_train_onehot.shape[1],    \n",
    "        input_dim=50,\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        bias_initializer='zeros',\n",
    "        activation='softmax'))\n",
    "\n",
    "\n",
    "sgd_optimizer = keras.optimizers.SGD(\n",
    "        lr=0.001, decay=1e-7, momentum=.9)\n",
    "\n",
    "model.compile(optimizer=sgd_optimizer,\n",
    "              loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CellStrat - we are using mini-batch stochastic gradient descent here.\n",
    "#The validation_split parameter is especially handy since it will reserve 10 percent\n",
    "#of the training data (here, 6,000 samples) for validation after each epoch so that we\n",
    "#can monitor whether the model is overfitting during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "844/844 [==============================] - 1s 2ms/step - loss: 0.7225 - val_loss: 0.3717\n",
      "Epoch 2/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.3777 - val_loss: 0.2826\n",
      "Epoch 3/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.3126 - val_loss: 0.2432\n",
      "Epoch 4/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2754 - val_loss: 0.2185\n",
      "Epoch 5/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2493 - val_loss: 0.2015\n",
      "Epoch 6/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2291 - val_loss: 0.1880\n",
      "Epoch 7/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2129 - val_loss: 0.1773\n",
      "Epoch 8/50\n",
      "844/844 [==============================] - 1s 2ms/step - loss: 0.1991 - val_loss: 0.1692\n",
      "Epoch 9/50\n",
      "844/844 [==============================] - 1s 2ms/step - loss: 0.1873 - val_loss: 0.1615\n",
      "Epoch 10/50\n",
      "844/844 [==============================] - 1s 2ms/step - loss: 0.1769 - val_loss: 0.1551\n",
      "Epoch 11/50\n",
      "844/844 [==============================] - 1s 2ms/step - loss: 0.1678 - val_loss: 0.1503\n",
      "Epoch 12/50\n",
      "844/844 [==============================] - 1s 2ms/step - loss: 0.1597 - val_loss: 0.1452\n",
      "Epoch 13/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.1524 - val_loss: 0.1419\n",
      "Epoch 14/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.1456 - val_loss: 0.1375\n",
      "Epoch 15/50\n",
      "844/844 [==============================] - 2s 2ms/step - loss: 0.1394 - val_loss: 0.1344\n",
      "Epoch 16/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.1337 - val_loss: 0.1318\n",
      "Epoch 17/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.1283 - val_loss: 0.1287\n",
      "Epoch 18/50\n",
      "844/844 [==============================] - 1s 2ms/step - loss: 0.1232 - val_loss: 0.1266\n",
      "Epoch 19/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.1188 - val_loss: 0.1252\n",
      "Epoch 20/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.1145 - val_loss: 0.1233\n",
      "Epoch 21/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.1104 - val_loss: 0.1217\n",
      "Epoch 22/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.1065 - val_loss: 0.1209\n",
      "Epoch 23/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.1031 - val_loss: 0.1194\n",
      "Epoch 24/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.0996 - val_loss: 0.1179\n",
      "Epoch 25/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.0963 - val_loss: 0.1176\n",
      "Epoch 26/50\n",
      "844/844 [==============================] - 2s 2ms/step - loss: 0.0932 - val_loss: 0.1160\n",
      "Epoch 27/50\n",
      "844/844 [==============================] - 1s 2ms/step - loss: 0.0902 - val_loss: 0.1164\n",
      "Epoch 28/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.0874 - val_loss: 0.1149\n",
      "Epoch 29/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.0848 - val_loss: 0.1146\n",
      "Epoch 30/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.0822 - val_loss: 0.1144\n",
      "Epoch 31/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.0798 - val_loss: 0.1140\n",
      "Epoch 32/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.0774 - val_loss: 0.1138\n",
      "Epoch 33/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.0753 - val_loss: 0.1133\n",
      "Epoch 34/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.0730 - val_loss: 0.1126\n",
      "Epoch 35/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.0710 - val_loss: 0.1129\n",
      "Epoch 36/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.0690 - val_loss: 0.1133\n",
      "Epoch 37/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.0670 - val_loss: 0.1125\n",
      "Epoch 38/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.0652 - val_loss: 0.1122\n",
      "Epoch 39/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.0634 - val_loss: 0.1123\n",
      "Epoch 40/50\n",
      "844/844 [==============================] - 2s 2ms/step - loss: 0.0616 - val_loss: 0.1119\n",
      "Epoch 41/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.0599 - val_loss: 0.1125\n",
      "Epoch 42/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.0583 - val_loss: 0.1125\n",
      "Epoch 43/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.0568 - val_loss: 0.1140\n",
      "Epoch 44/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.0553 - val_loss: 0.1140\n",
      "Epoch 45/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.0537 - val_loss: 0.1134\n",
      "Epoch 46/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.0524 - val_loss: 0.1142\n",
      "Epoch 47/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.0510 - val_loss: 0.1139\n",
      "Epoch 48/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.0497 - val_loss: 0.1140\n",
      "Epoch 49/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.0484 - val_loss: 0.1147\n",
      "Epoch 50/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.0471 - val_loss: 0.1154\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_centered, y_train_onehot,\n",
    "                    batch_size=64, epochs=50,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CellStrat - let's predict some values using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-7243619f5d55>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "First 3 predictions:  [5 0 4]\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = model.predict_classes(X_train_centered, verbose=0)\n",
    "print('First 3 predictions: ', y_train_pred[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CellStrat - print accuracy of predictions for training data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 predictions:  [5 0 4]\n",
      "Training accuracy: 98.89%\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = model.predict_classes(X_train_centered, \n",
    "                                     verbose=0)\n",
    "correct_preds = np.sum(y_train == y_train_pred, axis=0) \n",
    "train_acc = correct_preds / y_train.shape[0]\n",
    "\n",
    "print('First 3 predictions: ', y_train_pred[:3])\n",
    "print('Training accuracy: %.2f%%' % (train_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 96.35%\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model.predict_classes(X_test_centered, \n",
    "                                    verbose=0)\n",
    "\n",
    "correct_preds = np.sum(y_test == y_test_pred, axis=0) \n",
    "test_acc = correct_preds / y_test.shape[0]\n",
    "print('Test accuracy: %.2f%%' % (test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
